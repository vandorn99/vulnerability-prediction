#%%
import pandas as pd
import json
import os
import re
from Naked.toolshed.shell import execute_js
#%%
def get_vuln_lines(hash_value):
    found = list()
    try:
        os.chdir(f'../patches/{hash_value}')
        with open(os.listdir()[0], 'r') as fp:
            for line in fp.readlines():
                if line.startswith('-') and not line.startswith('---'):
                    if len(line)>1000:
                        print(hash_value)
                    else:
                        line = '-'.join(line.split('-')[1:]).strip()
                        if re.search('[A-Za-z0-9]', line) and not line.startswith('//') and not line.startswith('*'):
                            found.append(line)
    except FileNotFoundError as ex:
        print(ex)
    os.chdir(BASEDIR)
    return found
# %%
def get_split(seed, df):
    tests =  df.sample(frac=0.05, random_state=seed)

    git_paths = list(tests['full_repo_path'].map(lambda x : '/'.join(x.split('/')[:5]) + '.git' if x.startswith('https://') else 'https://'+'/'.join(x.split('/')[:3]) + '.git'))
    parent_hashes = list(tests['full_repo_path'].map(lambda x : x.split('/')[6] if x.startswith('https://') else x.split('/')[4]))
    commit_hashes = [test if not '-' in test else test.split('-')[0] for test in tests['Fix_hash']]
    paths = list()
    for git_path, path in zip(git_paths, list(tests['path'])):
        paths.append('.'.join(git_path.split('/')[4].split('.')[:-1]) + '/' + path)
    test_results = list()

    for git_path, commit_hash, parent_hash, path in zip(git_paths, commit_hashes, parent_hashes, paths):
        test_results.append(
            {
                'git_path' : git_path,
                'target_file_path' : path,
                'commit_hash' : commit_hash,
                'parent_hash' : parent_hash
            }
        )
    train = pd.concat([df,tests]).drop_duplicates(keep=False)
    train['project_name'] = train['full_repo_path'].map(lambda x : '/'.join(x.split('/')[:-4]))
    hashes = df['Fix_hash']

    train_lines = list()

    for hash_val in hashes:
        hash_val = hash_val.split('-')
        for value in hash_val:
            train_lines.extend(get_vuln_lines(value))
    return (train_lines, test_results)

#%%

df = pd.read_csv('../vuln_func_raise.csv')
df = df.dropna(axis=1, how='all').drop('Unnamed: 47', axis=1).dropna().drop_duplicates(subset=['Fix_hash'])
BASEDIR = os.getcwd()
model_names = ['IDname', 'SpecialValues', 'novarname']
with open('../word2vec/keywords.txt', 'r') as k:
    keywords = ''
    for line in k.readlines():
        keywords += line.rstrip() + ','
k = 10
# %%
for i in range(1,k+1):
    (train, test) = get_split(42*i,df)
    with open(f'targets{i}.json', 'w') as fp:
        json.dump(test, fp, indent=3)
    with open('temp.txt', 'w') as fp:
        for line in train:
            fp.write(line + '\n')
    for modelName in model_names:
        execute_js(f'./js_subscripts/lexfile/{modelName}_lexfile.js ./temp.txt {keywords}')
        with open('lexed.txt', 'r') as fp:
            with open(f'vuln_lines_{modelName}_{i}.txt', 'w') as fp2:
                for line in fp.readlines():
                    fp2.write(line)
# %%
