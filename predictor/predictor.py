import csv
import argparse
import os
import numpy as np
import gensim.models.word2vec as wv
import json
import subprocess
import re
import math

from copy import deepcopy
from Naked.toolshed.shell import muterun_js, execute_js
from scipy import spatial
from concurrent.futures import ThreadPoolExecutor
from concurrent.futures import wait

vuln_line_vectors = list()
acceptance = list()
model = None
index2word_set = set()
vuln_lines = list()
BASEDIR = os.getcwd()

def get_dev_block_values(start, end):
    length = (end-start)+1
    results = [0] * length
    if length % 2:
        center = math.floor(length/2)
        results[center] = length
        for i in range(1, center+1):
            results[center+i] = results[center-i] = length - i
    else:
        center = int(length/2)
        for i in range(0, center):
            results[center+i] = results[center-(i+1)] = length - i
    return results

# create a vector that's as long as the amount of features we have(may be an issue)
# if a word is in our model add it to the sentece's feature vector
# get the average
def avg_feature_vector(sentence, model, num_features, index2word_set):
    words = sentence.split()
    feature_vec = np.zeros((num_features, ), dtype='float32')
    n_words = 0
    for word in words:
        if word in index2word_set:
            n_words += 1
            feature_vec = np.add(feature_vec, model[word])
    if (n_words > 0):
        feature_vec = np.divide(feature_vec, n_words)
    return feature_vec

# get the lines minimum distance from one of the already stored vulnerable lines
# return a tuple of the vuln value and the line that's closest
def get_line_vuln(line, model, features, index2word_set):
    mini = 1
    reason = ''
    line_vec = avg_feature_vector(line, model, features, index2word_set)
    for vec, line in zip(vuln_line_vectors, vuln_lines):
        curr = spatial.distance.cosine(line_vec, vec)
        if curr < mini:
            mini = curr
            reason = line
    return (mini,reason)

# runs a js subscript that extracts the containing function
def get_container_func(line_data, path):
    return muterun_js(f'./js_subscripts/method-dispenser/index.js {path[2:]} {line_data["line_num"]}')

# predict wether a line is vulnerable or not
def predict(target, keywords, model, features, index2word_set, acceptance, file_name=None, modelName=None, rules=list(), count = -1):
    line_results = list()

    # if the file doesn't exist fail here
    with open(target, 'r'):
        pass

    # lex the file with the current model's corresponding lexer
    execute_js(f'./js_subscripts/lexfile/{(modelName+"_" if modelName else "IDname_")}lexfile.js {target} {keywords}')
    # calc all vuln line vectors
    # create a list of dict's where each dict contains the line, the chance of it being vulnerable, the reason for that given chance, and the lines number
    with open(target, 'r', errors='ignore') as fp:
        with open('./lexed.txt','r') as fp2:
            line_num = 0
            for line, lexed_line in zip(fp.readlines(),fp2.readlines()):
                (line_val,reason) = get_line_vuln(lexed_line, model, features, index2word_set)
                line_results.append(
                    {
                        'line_num' : line_num,
                        'vuln_prob' : 1 - line_val,
                        'line' : line,
                        'reason' : reason,
                        'distance': 1 - line_val,
                        'surrounding': 0,
                        'complexity': 0,
                        'git_surr' : 0,
                        'model': modelName
                    }
                )
                line_num += 1
    if file_name:
        os.chdir(f'./{modelName}_results'+('' if count == -1 else '_'+str(count))+f'/{file_name}')
    # perform the check for every specified acceptance value
    with ThreadPoolExecutor(max_workers=os.cpu_count() + 4) as executor:
        accept_results = dict()
        for accept in acceptance:
            accept_results[accept] = executor.submit(execute_rules, accept, line_results, rules, target)

        for accept in acceptance:
            # any rules that change the cwd are executed here
            # they are only executed if the base rule list contains them
            if 'rule_check_change_blocks' in accept_results[accept].result().keys():
                accept_results[accept].result()['check_change_blocks'] = [line for line in check_change_blocks(line_results, accept, target) if line['vuln_prob'] > accept]
                
        for accept in accept_results:
            if file_name:
                if not os.path.isdir(f'{int(accept*100)}%'):
                        os.mkdir(f'{int(accept*100)}%')

                os.chdir(f'{int(accept*100)}%')
            vuln_lines_processing_futures = list()
            for rule_name in accept_results[accept].result():
                vuln_lines_processing_futures.append(executor.submit(process_vuln_lines, accept_results[accept].result()[rule_name], rule_name))
            #this is needed so the vuln_line processing tasks finish while the cwd is in the desired location
            wait(vuln_lines_processing_futures)
            os.chdir('../')
        os.chdir(BASEDIR)

def execute_rules(accept, line_results, rules, target):
    rule_results = dict()

    # create a new list of vulnerable lines
    vuln_lines = [line for line in line_results if line['vuln_prob'] > accept]
    # before any rules are applied create output for the base results
    rule_results['no_rule'] = deepcopy(vuln_lines)
    # apply each specified rule
    for rule in rules:
        # this needs to be extended for every rule that changes cwd during it's runtime
        if rule.__name__ is not 'check_change_blocks':
            vuln_lines = [line for line in rule(line_results, accept, target) if line['vuln_prob'] > accept]
        rule_results[f'rule_{rule.__name__}'] = deepcopy(vuln_lines)

    return rule_results

# create the output cve file
def process_vuln_lines(vuln_lines, file_name):
    func_results = list()

    '''for vuln_line in vuln_lines:
        container = get_container_func(vuln_line, target).stdout.strip()
        if container not in func_results:
            func_results.append(container)'''


    with open(('sus.csv' if not file_name else f'{file_name}.csv'),'w', newline='') as fp:
        writer = csv.DictWriter(fp, fieldnames = ['num','prob','line','reason', 'distance', 'surrounding', 'complexity', 'git_surr'])
        writer.writerow({
                        'num' : 'Line_number',
                        'prob' : 'Chance_of_vuln',
                        'line' : 'Line_content',
                        'reason' : 'Reason_for_pred',
                        'distance': 'distance',
                        'surrounding': 'surrounding',
                        'complexity': 'complexity',
                        'git_surr' : 'git_surr'
                    })
        for line in vuln_lines:
            writer.writerow({
                        'num' : line['line_num'],
                        'prob' : line['vuln_prob'],
                        'line' : line['line'],
                        'reason' : line['reason'],
                        'distance': line['distance'],
                        'surrounding': line['surrounding'],
                        'complexity': line['complexity'],
                        'git_surr' : line['git_surr']
                    })
    print('CSV done')

# reads in all the given vulnerable lines
def readVulnLines(model, features, index2word_set, file_name='vuln_lines_new_lex.txt'):
    with open(file_name, 'r') as fp:
        for line in fp.readlines():
            vuln_lines.append(line)
            vuln_line_vectors.append(avg_feature_vector(line, model, features, index2word_set))

# this function servers to psuh values back to between 0-1
def weight_func(x, n=0):
    return 1 - ((1 + n)/(x + n))

# filters out all lines that only contain 1 word, this is applied permanently, and so it needs to be the first rule
def no_one_word_lines(line_data, acceptance, target):
    for line in line_data:
        if len(line['line'].split()) < 2:
            line['vuln_prob'] = 0
    return line_data
# checks how many vulnerable lines there are in the same vuln_block as the line given
def check_surroundings(line_data, acceptance, target):
    result = deepcopy(line_data)

    for (i, line), data in zip(enumerate(result), line_data):
        neighbourCount = 1
        if data['vuln_prob'] > acceptance:
            for j in range(i-1,0,-1):
                if line_data[j]['vuln_prob'] > acceptance:
                    neighbourCount += 1
                else:
                    break
            for j in range(i+1, len(line_data)-1):
                if line_data[j]['vuln_prob'] > acceptance:
                    neighbourCount += 1
                else:
                    break

            line['vuln_prob'] = (line['vuln_prob'] + weight_func(neighbourCount)) / 2
            line['surrounding'] = neighbourCount

    return result

# prefer more complex lines
def prefer_complex(line_data, acceptance, target):
    result = deepcopy(line_data)

    lexed_lines = list()
    with open(BASEDIR + '/lexed.txt', 'r') as fp:
        lexed_lines = fp.readlines()
    for line in result:

        if(len(set(lexed_lines[line['line_num']].split()))):
            line['vuln_prob'] = (line['vuln_prob'] + weight_func(len(set(lexed_lines[line['line_num']].split())))) / 2
            line['complexity'] = len(set(lexed_lines[line['line_num']].split()))

    return result

# combine prefer_complex and check_surroundings, here the complexity preference is applied after the vuln_blocks are calculated
def complex_and_surrounded(line_data, acceptance, target):
    result = deepcopy(line_data)

    lexed_lines = list()
    with open(BASEDIR + '/lexed.txt', 'r') as fp:
        lexed_lines = fp.readlines()

    for (i, line), data in zip(enumerate(result),line_data):
        neighbourCount = 1
        if data['vuln_prob'] > acceptance:
            for j in range(i-1,0,-1):
                if line_data[j]['vuln_prob'] > acceptance:
                    neighbourCount += 1
                else:
                    break
            for j in range(i+1, len(line_data)-1):
                if line_data[j]['vuln_prob'] > acceptance:
                    neighbourCount += 1
                else:
                    break
            if(len(set(lexed_lines[line['line_num']].split()))):
                line['vuln_prob'] = (line['vuln_prob'] + weight_func(len(set(lexed_lines[line['line_num']].split()))) + weight_func(neighbourCount)) / 3
                line['complexity'] = len(set(lexed_lines[line['line_num']].split()))
                line['surrounding'] = neighbourCount

    return result

# apply the check surroundings rule multiple times, to see if it converges
def iter_surrounding(line_data, acceptance, target):
    result = deepcopy(line_data)

    for (i, line), data in zip(enumerate(result), line_data):
        neighbourCount = 1
        if data['vuln_prob'] > acceptance:
            for j in range(i-1,0,-1):
                if line_data[j]['vuln_prob'] > acceptance:
                    neighbourCount += 1
                else:
                    break
            for j in range(i+1, len(line_data)-1):
                if line_data[j]['vuln_prob'] > acceptance:
                    neighbourCount += 1
                else:
                    break

            line['vuln_prob'] = (line['vuln_prob'] + weight_func(neighbourCount)) / 2
            line['surrounding'] = (line['surrounding'] + neighbourCount) / 2

    for _ in range(0,100):
        prev_state = deepcopy(result)
        for (i, line), data in zip(enumerate(result), prev_state):
            neighbourCount = 1
            if data['vuln_prob'] > acceptance:
                for j in range(i-1,0,-1):
                    if line_data[j]['vuln_prob'] > acceptance:
                        neighbourCount += 1
                    else:
                        break
                for j in range(i+1, len(line_data)-1):
                    if line_data[j]['vuln_prob'] > acceptance:
                        neighbourCount += 1
                    else:
                        break

                line['vuln_prob'] = (line['vuln_prob'] + weight_func(neighbourCount)) / 2
                line['surrounding'] = neighbourCount

    return result

# combine prefer_complex and check_surroundings, apply the complex preference before vuln_blocks are calculated
def complex_before_surr(line_data, acceptance, target):
    result = deepcopy(line_data)

    lexed_lines = list()
    with open(BASEDIR + '/lexed.txt', 'r') as fp:
        lexed_lines = fp.readlines()

    for line in result:
        if(len(set(lexed_lines[line['line_num']].split()))):
            line['vuln_prob'] = (line['vuln_prob'] + weight_func(len(set(lexed_lines[line['line_num']].split())))) / 2
            line['complexity'] = len(set(lexed_lines[line['line_num']].split()))

    for (i, line), data in zip(enumerate(result),line_data):
        neighbourCount = 1
        if data['vuln_prob'] > acceptance:
            for j in range(i-1,0,-1):
                if line_data[j]['vuln_prob'] > acceptance:
                    neighbourCount += 1
                else:
                    break
            for j in range(i+1, len(line_data)-1):
                if line_data[j]['vuln_prob'] > acceptance:
                    neighbourCount += 1
                else:
                    break
            line['vuln_prob'] = (line['vuln_prob'] + weight_func(neighbourCount)) / 2
            line['surrounding'] = neighbourCount

    return result

def complex_before_surr_reg(line_data, acceptance, target):
    result = deepcopy(line_data)

    with open(BASEDIR + '/lexed.txt', 'r') as fp:
        lexed_lines = fp.readlines()

    with open(BASEDIR + '/coeffs.json', 'r') as fp:
        coeffs = json.load(fp)

    model_name = ''
    for line in result:
        if (len(set(lexed_lines[line['line_num']].split()))):
            model_name = line['model']
            line['vuln_prob'] = (line['vuln_prob'] + weight_func(len(set(lexed_lines[line['line_num']].split())))) / 2
            line['complexity'] = len(set(lexed_lines[line['line_num']].split()))

    selected_values = ''
    for key in coeffs.keys():
        if str(key).startswith(model_name) and str(key).endswith('before_surr.csv'):
            selected_values = coeffs[key]
            break

    parts = selected_values.split('+')
    constant = float(parts[0].strip())
    coeff_distance = float(parts[1].split('*')[0].strip())
    coeff_surrounding = float(parts[2].split('*')[0].strip())
    coeff_complexity = float(parts[3].split('*')[0].strip())

    for (i, line), data in zip(enumerate(result), line_data):
        neighbourCount = 1
        if data['vuln_prob'] > 0:
            for j in range(i - 1, 0, -1):
                if line_data[j]['vuln_prob'] > 0:
                    neighbourCount += 1
                else:
                    break
            for j in range(i + 1, len(line_data) - 1):
                if line_data[j]['vuln_prob'] > 0:
                    neighbourCount += 1
                else:
                    break
            line['vuln_prob'] = (line['vuln_prob'] + weight_func(neighbourCount)) / 2
            line['surrounding'] = neighbourCount

    for line in result:
        if constant + coeff_distance * line['distance'] + coeff_surrounding * line['surrounding'] + coeff_complexity * line['complexity'] > acceptance:
            line['vuln_prob'] = 1
        else:
            line['vuln_prob'] = 0

    return result

def complex_and_surrounded_reg(line_data, acceptance, target):
    result = deepcopy(line_data)

    with open(BASEDIR + '/lexed.txt', 'r') as fp:
        lexed_lines = fp.readlines()

    with open(BASEDIR + '/coeffs.json', 'r') as fp:
        coeffs = json.load(fp)

    model_name = ''
    for line in result:
        model_name = line['model']
        break

    selected_values = ''
    for key in coeffs.keys():
        if str(key).startswith(model_name) and str(key).endswith('surrounded.csv'):
            selected_values = coeffs[key]
            break

    parts = selected_values.split('+')
    constant = float(parts[0].strip())
    coeff_distance = float(parts[1].split('*')[0].strip())
    coeff_surrounding = float(parts[2].split('*')[0].strip())
    coeff_complexity = float(parts[3].split('*')[0].strip())

    for (i, line), data in zip(enumerate(result), line_data):
        neighbourCount = 1
        if data['vuln_prob'] > 0:
            for j in range(i - 1, 0, -1):
                if line_data[j]['vuln_prob'] > 0:
                    neighbourCount += 1
                else:
                    break
            for j in range(i + 1, len(line_data) - 1):
                if line_data[j]['vuln_prob'] > 0:
                    neighbourCount += 1
                else:
                    break
            if (len(set(lexed_lines[line['line_num']].split()))):
                line['vuln_prob'] = (line['vuln_prob'] + weight_func(
                    len(set(lexed_lines[line['line_num']].split()))) + weight_func(neighbourCount)) / 3
                line['complexity'] = len(set(lexed_lines[line['line_num']].split()))
                line['surrounding'] = neighbourCount

    for line in result:
        if constant + coeff_distance * line['distance'] + coeff_surrounding * line['surrounding'] + coeff_complexity * line['complexity'] > acceptance:
            line['vuln_prob'] = 1
        else:
            line['vuln_prob'] = 0

    return result

def check_change_blocks(line_data, acceptance, target):
    result = deepcopy(line_data)

    current_dir = os.getcwd()

    os.chdir(BASEDIR)

    path = '/'.join(target.split('/')[:-1])

    filename = target.split('/')[-1]

    os.chdir(path)

    lines  = subprocess.getoutput(f'git blame {filename}').split('\n')
    names = list()

    for line in lines:
        pattern = re.compile(r'[0-9]{4}-[0-9]{2}-[0-9]{2}')
        line_elements = line.split()
        endName = [i for i, element in enumerate(line_elements) if pattern.match(element)]
        if len(endName) == 1:
            names.append(' '.join(line_elements[1:endName[0]]))
        else:
            print("ERROR")
            print(endName)

    blockValues = list()

    start = -1
    end = 0
    prev = ''

    for i,name in enumerate(names):
        if start == -1:
            start = 0
            prev = name
        elif i == len(names)-1:
            blockValues += get_dev_block_values(start, end)
        else:
            if prev == name:
                end = i
            else:
                if start == end:
                    blockValues.append(1)
                else:
                    blockValues += get_dev_block_values(start, end)
                start = i
                end = i
                prev = name

    os.chdir(current_dir)

    for res, value in zip(result, blockValues):
        res['vuln_prob'] = (res['vuln_prob'] + (1 - weight_func(value, 10))) / 2
        res['git_surr'] = value

    return result

def new_surrounded(line_data, acceptance, target):
    result = deepcopy(line_data)

    lexed_lines = list()
    with open(BASEDIR + '/lexed.txt', 'r') as fp:
        lexed_lines = fp.readlines()

    for line in result:
        if(len(set(lexed_lines[line['line_num']].split()))):
            line['vuln_prob'] = (line['vuln_prob'] + weight_func(len(set(lexed_lines[line['line_num']].split())))) / 2
            line['complexity'] = len(set(lexed_lines[line['line_num']].split()))

    for (i, line), data in zip(enumerate(result),line_data):
        neighbourCount = 1
        if data['vuln_prob'] > acceptance:
            for j in range(i-1,0,-1):
                if line_data[j]['vuln_prob'] > acceptance:
                    neighbourCount += 1
                else:
                    break
            for j in range(i+1, len(line_data)-1):
                if line_data[j]['vuln_prob'] > acceptance:
                    neighbourCount += 1
                else:
                    break
            if neighbourCount > 0:
                line['vuln_prob'] = (line['vuln_prob'] + weight_func(neighbourCount)) / 2
            else:
                line['vuln_prob'] = 0
            line['surrounding'] = neighbourCount

    return result

def main():
    # get target path from command line argument
    arg_parser = argparse.ArgumentParser(description='Look for potentially vulnreable lines in a js file')
    arg_parser.add_argument('-f', '--target_file', metavar='', help='Path to the file that needs to be checked')
    args = arg_parser.parse_args()

    # these are treated as global varaibles in case they would be needed for a rule
    global acceptance, model, index2word_set

    # js keywords are read in from a txt file
    with open('../word2vec/keywords.txt', 'r') as k:
        keywords = ''
        for line in k.readlines():
            keywords += line.rstrip() + ','

    # the predictor should be runnable for a specific file, as for now, this uses a preset of the IDname model, 80% acceptance, and no rules
    if args.target_file == 'k_fold':
        k = 10
        # for the creation of test results every available option is explored herefor acceptance values, rules, and models
        acceptance = [0.92]

        # the modelNames need to be in the same order as the txt files
        modelNames = ['SpecialValues']

        not_found = list()

        features = 100

        rules = [no_one_word_lines, check_surroundings, prefer_complex, complex_and_surrounded, complex_before_surr, iter_surrounding, new_surrounded]

        for i in range(1,k+1):
            with open(f'target_paths{i}.json','r') as fp:
                targets = json.load(fp)

            names = dict()

            for mName in modelNames:
                model = wv.Word2Vec.load(f"models/{mName}.model")
                file_name = f'vuln_lines_{mName}_{i}.txt'
                index2word_set = set(model.wv.index2word)

                if not os.path.isdir(f'./{mName}_results_{i}'):
                    os.mkdir(f'./{mName}_results_{i}')
                readVulnLines(model, features, index2word_set, file_name)
                for target in targets:

                    target_name = target.split('/')[-1]

                    if target_name not in names:
                        names[target_name] = 0
                    else:
                        names[target_name] += 1
                        target_name += str(names[target_name])

                    if not os.path.isdir(f'./{mName}_results_{i}/{target_name}'):
                        os.mkdir(f'./{mName}_results_{i}/{target_name}')
                    print(f'Begin {mName} test on {target_name}')
                    try:
                        predict(target, keywords, model, features, index2word_set, acceptance, target_name, mName, rules, i)
                    except FileNotFoundError as er:
                        not_found.append(er)
        print(len(not_found))
        print(not_found)

    elif args.target_file == 'create_test_results':
        # for the creation of test results every available option is explored herefor acceptance values, rules, and models
        acceptance = [0.75, 0.8, 0.85, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97]

        # the modelNames need to be in the same order as the txt files
        modelNames = ['SpecialValues','IDname', 'novarname']

        vuln_file_names = ['vuln_lines_extended_lex.txt', 'vuln_lines_new_lex.txt', 'vuln_lines_old_lex.txt']

        features = 100

        rules = [no_one_word_lines, check_surroundings, prefer_complex, complex_and_surrounded, complex_before_surr, iter_surrounding, new_surrounded]



        with open('target_paths.json','r') as fp:
            targets = json.load(fp)

        for mName, file_name in zip(modelNames, vuln_file_names):
            model = wv.Word2Vec.load(f"models/{mName}.model")

            index2word_set = set(model.wv.index2word)

            http_server_count = 1

            if not os.path.isdir(f'./{mName}_results'):
                os.mkdir(f'./{mName}_results')
            readVulnLines(model, features, index2word_set, file_name)
            for target in targets:

                target_name = target.split('/')[-1]
                
                if target_name == '_http_server.js':
                    target_name = (target_name if http_server_count == 1 else '_http_server' + str(http_server_count) + '.js')
                    http_server_count += 1

                if not os.path.isdir(f'./{mName}_results/{target_name}'):
                    os.mkdir(f'./{mName}_results/{target_name}')
                print(f'Begin {mName} test on {target_name}')
                predict(target, keywords, model, features, index2word_set, acceptance, target_name, mName, rules)
        else:
            acceptance = [0.8]

            model = wv.Word2Vec.load("models/IDname.model")

            features = 100

            index2word_set = set(model.wv.index2word)

            readVulnLines(model, features, index2word_set)

            predict(args.target_file, keywords, model, features, index2word_set, acceptance)


if __name__ == '__main__':
    main()
